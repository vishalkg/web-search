# WebSearch MCP Server

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Pylint Score](https://img.shields.io/badge/pylint-10.00/10-brightgreen)](https://pylint.org/)

High-performance Model Context Protocol (MCP) server for web search and content extraction with intelligent fallback system.

## âœ¨ Features

- **ğŸš€ Fast**: Async implementation with parallel execution
- **ğŸ” Multi-Engine**: Google, Bing, DuckDuckGo, Startpage, Brave Search
- **ğŸ›¡ï¸ Intelligent Fallbacks**: Googleâ†’Startpage, Bingâ†’DuckDuckGo, Brave (standalone)
- **ğŸ“„ Content Extraction**: Clean text extraction from web pages
- **ğŸ’¾ Smart Caching**: LRU cache with compression and deduplication
- **ğŸ”‘ API Integration**: Google Custom Search, Brave Search APIs with quota management
- **âš¡ Resilient**: Automatic failover and comprehensive error handling

## ğŸš€ Quick Start

### Q CLI
```bash
# Install from GitHub
git clone https://github.com/vishalkg/web-search.git ~/.mcp/web-search
cd ~/.mcp
python3 -m venv venv
source venv/bin/activate
cd web-search
pip install -e .
chmod +x start.sh

# Add to Q CLI
q mcp add websearch ~/.mcp/web-search/start.sh

# Test
q chat "search for python tutorials"
```

## ğŸ“¦ Installation Options

### Option 1: Direct install from GitHub (Recommended)
```bash
# Basic installation (DuckDuckGo, Bing, Startpage)
pip install git+https://github.com/vishalkg/web-search.git

# With Google API support  
pip install "git+https://github.com/vishalkg/web-search.git[google]"

# With all features
pip install "git+https://github.com/vishalkg/web-search.git[all]"

# Then run
websearch-server
```

### Option 2: Development installation
```bash
git clone https://github.com/vishalkg/web-search.git
cd web-search
pip install -e .
```

### Option 3: Manual dependencies
```bash
git clone https://github.com/vishalkg/web-search.git
cd web-search
pip install -r requirements.txt
```

## ğŸ—‚ï¸ File Structure (Installation Independent)

The server automatically creates and manages files in a unified user directory:

```
~/.websearch/                 # Single websearch directory
â”œâ”€â”€ venv/                    # Virtual environment (recommended)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ .env                 # Configuration file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ search-metrics.jsonl # Search analytics
â”‚   â””â”€â”€ quota/              # API quota tracking
â”‚       â”œâ”€â”€ google_quota.json
â”‚       â””â”€â”€ brave_quota.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ web-search.log      # Application logs
â””â”€â”€ cache/                  # Optional caching
```

### Environment Variable Overrides
- `WEBSEARCH_HOME`: Base directory (default: `~/.websearch`)
- `WEBSEARCH_CONFIG_DIR`: Config directory override  
- `WEBSEARCH_LOG_DIR`: Log directory override

## âš™ï¸ MCP Configuration

### Recommended Setup (unified directory):
```bash
# Create everything in ~/.websearch/
python -m venv ~/.websearch/venv
source ~/.websearch/venv/bin/activate
pip install git+https://github.com/vishalkg/web-search.git
```

### MCP Settings:
```json
{
  "mcpServers": {
    "websearch": {
      "command": "~/.websearch/venv/bin/websearch-server"
    }
  }
}
```

### Alternative (system/user install):
```json
{
  "mcpServers": {
    "websearch": {
      "command": "websearch-server"
    }
  }
}
```
q chat
# Try: "search web for python tutorials"
```

### Claude Desktop
```bash
# Install and configure (see detailed instructions below)
git clone https://github.com/vishalkg/web-search.git ~/.mcp/web-search
# Configure claude_desktop_config.json
# Restart Claude Desktop
# Look for ğŸ”¨ MCP indicator
```

## ğŸ”§ Configuration

### API Keys (Optional)
For enhanced performance, configure API keys:

```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your API keys
GOOGLE_CSE_API_KEY=your_google_api_key_here
GOOGLE_CSE_ID=your_google_cse_id_here
BRAVE_SEARCH_API_KEY=your_brave_api_key_here
```

**Fallback Behavior:**
- **Google API** â†’ Falls back to **Startpage** scraping if quota exhausted
- **Bing scraping** â†’ Falls back to **DuckDuckGo** scraping if blocked
- **Brave API** â†’ Standalone with quota management

## ğŸ“¦ Installation
cd ~/.mcp
python3 -m venv venv
source venv/bin/activate
cd web-search
pip install -e .
chmod +x start.sh
q mcp add websearch ~/.mcp/web-search/start.sh
```

### For Claude Desktop
```bash
# 1. Install the server
git clone https://github.com/vishalkg/web-search.git ~/.mcp/web-search
cd ~/.mcp/web-search
python3 -m venv venv
source venv/bin/activate
pip install -e .

# 2. Configure Claude Desktop
# Open Claude Desktop â†’ Settings â†’ Developer â†’ Edit Config
# Add this to your claude_desktop_config.json:
```

**macOS Configuration:**
```json
{
  "mcpServers": {
    "websearch": {
      "command": "/Users/YOUR_USERNAME/.mcp/web-search/venv/bin/python",
      "args": ["-m", "websearch.server"],
      "cwd": "/Users/YOUR_USERNAME/.mcp/web-search"
    }
  }
}
```

**Windows Configuration:**
```json
{
  "mcpServers": {
    "websearch": {
      "command": "C:\\Users\\YOUR_USERNAME\\.mcp\\web-search\\venv\\Scripts\\python.exe",
      "args": ["-m", "websearch.server"],
      "cwd": "C:\\Users\\YOUR_USERNAME\\.mcp\\web-search"
    }
  }
}
```

**After configuration:**
1. Replace `YOUR_USERNAME` with your actual username
2. Restart Claude Desktop completely
3. Look for the ğŸ”¨ MCP indicator in the chat input
4. Try: "search web for python tutorials"

## ğŸ”§ Usage

The server provides two main tools with multiple search modes:

### Search Web
```python
# Standard 5-engine search (backward compatible)
search_web("quantum computing applications", num_results=10)

# New 3-engine fallback search (optimized)
search_web_fallback("machine learning tutorials", num_results=5)
```

**Search Engines:**
- **Google Custom Search API** (with Startpage fallback)
- **Bing** (with DuckDuckGo fallback) 
- **Brave Search API** (standalone)
- **DuckDuckGo** (scraping)
- **Startpage** (scraping)

### Fetch Page Content  
```python
# Extract clean text from URLs
fetch_page_content("https://example.com")
fetch_page_content(["https://site1.com", "https://site2.com"])  # Batch processing
```

## ğŸ—ï¸ Architecture

```
websearch/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ search.py              # Sync search orchestration
â”‚   â”œâ”€â”€ async_search.py        # Async search orchestration  
â”‚   â”œâ”€â”€ fallback_search.py     # 3-engine fallback system
â”‚   â”œâ”€â”€ async_fallback_search.py # Async fallback system
â”‚   â”œâ”€â”€ ranking.py             # Quality-first result ranking
â”‚   â””â”€â”€ common.py              # Shared utilities
â”œâ”€â”€ engines/
â”‚   â”œâ”€â”€ google_api.py          # Google Custom Search API
â”‚   â”œâ”€â”€ brave_api.py           # Brave Search API
â”‚   â”œâ”€â”€ bing.py                # Bing scraping
â”‚   â”œâ”€â”€ duckduckgo.py          # DuckDuckGo scraping
â”‚   â””â”€â”€ startpage.py           # Startpage scraping
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ unified_quota.py       # Unified API quota management
â”‚   â”œâ”€â”€ deduplication.py       # Result deduplication
â”‚   â”œâ”€â”€ advanced_cache.py      # Enhanced caching system
â”‚   â””â”€â”€ http.py                # HTTP utilities
â””â”€â”€ server.py                  # FastMCP server
```

## ğŸ”§ Advanced Configuration

### Environment Variables
```bash
# API Configuration
export GOOGLE_CSE_API_KEY=your_google_api_key
export GOOGLE_CSE_ID=your_google_cse_id  
export BRAVE_SEARCH_API_KEY=your_brave_api_key

# Quota Management (Optional)
export GOOGLE_DAILY_QUOTA=100        # Default: 100 requests/day
export BRAVE_MONTHLY_QUOTA=2000      # Default: 2000 requests/month

# Performance Tuning
export WEBSEARCH_CACHE_SIZE=1000
export WEBSEARCH_TIMEOUT=10
export WEBSEARCH_LOG_LEVEL=INFO
```

### Quota Management
- **Unified System**: Single quota manager for all APIs
- **Google**: Daily quota (default 100 requests/day)
- **Brave**: Monthly quota (default 2000 requests/month)
- **Storage**: Quota files stored in `~/.websearch/` directory
- **Auto-reset**: Quotas automatically reset at period boundaries
- **Fallback**: Automatic fallback to scraping when quotas exhausted

### Search Modes
- **Standard Mode**: Uses all 5 engines for maximum coverage
- **Fallback Mode**: Uses 3 engines with intelligent fallbacks for efficiency
- **API-First Mode**: Prioritizes API calls over scraping when keys available

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| No results | Check internet connection and logs |
| API quota exhausted | System automatically falls back to scraping |
| Google API errors | Verify `GOOGLE_CSE_API_KEY` and `GOOGLE_CSE_ID` |
| Brave API errors | Check `BRAVE_SEARCH_API_KEY` and quota status |
| Permission denied | `chmod +x start.sh` |
| Import errors | Ensure Python 3.12+ and dependencies installed |
| Circular import warnings | Fixed in v2.0+ (10.00/10 pylint score) |

### Debug Mode
```bash
# Enable detailed logging
export WEBSEARCH_LOG_LEVEL=DEBUG
python -m websearch.server
```

### API Status Check
```bash
# Test API connectivity
cd debug/
python test_brave_api.py      # Test Brave API
python test_fallback.py       # Test fallback system
```

## ğŸ“ˆ Performance & Monitoring

### Metrics
- **Pylint Score**: 10.00/10 (perfect code quality)
- **Search Speed**: ~2-3 seconds for 5-engine search
- **Fallback Speed**: ~1-2 seconds for 3-engine search  
- **Cache Hit Rate**: ~85% for repeated queries
- **API Quota Efficiency**: Automatic fallback prevents service interruption

### Monitoring
Logs are written to `web-search.log` with structured format:
```bash
tail -f web-search.log | grep "search completed"
```

## ğŸ”’ Security

- **No hardcoded secrets**: All API keys via environment variables
- **Clean git history**: Secrets scrubbed from all commits
- **Input validation**: Comprehensive sanitization of search queries
- **Rate limiting**: Built-in quota management for API calls
- **Secure defaults**: HTTPS-only requests, timeout protection

## ğŸš€ Performance Tips

1. **Use fallback mode** for faster searches when you don't need maximum coverage
2. **Set API keys** to reduce reliance on scraping (faster + more reliable)
3. **Enable caching** for repeated queries (enabled by default)
4. **Tune batch sizes** for content extraction based on your needs

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests (`pytest`)
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push to branch (`git push origin feature/amazing-feature`)
6. Open Pull Request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.
